{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "CnZ6qQ60uOxv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "Oxb0aS-MuZ_I"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 1*\n",
        "Написать на PyTorch forward и backward полносвязного слоя без использования autograd"
      ],
      "metadata": {
        "id": "CnZ6qQ60uOxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyLayer(nn.Module):\n",
        "  def __init__(self, tensor):\n",
        "    super().__init__()\n",
        "    self.params = nn.Parameter(tensor, requires_grad=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x * self.params\n",
        "\n",
        "  def backward(self, grad):\n",
        "    \n",
        "    #todo\n",
        "    \n",
        "    return grad"
      ],
      "metadata": {
        "id": "bF4xjJqz_Tr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 2\n",
        "Написать 1-2 адаптивных оптимизатора"
      ],
      "metadata": {
        "id": "NCW7vOPEvZex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGDMomentum:\n",
        "  def __init__(self, model_weights, momentum: float=0.99, lr: float=0.001):\n",
        "    self.momentum = momentum\n",
        "    self.lr = lr\n",
        "    self.velocity = torch.zeros_like(model_weights)\n",
        "    self.weight = model_weights\n",
        "\n",
        "  def step(self, grad):\n",
        "    self.velocity = self.momentum * self.velocity - self.lr * grad\n",
        "    self.weight = self.weight + self.velocity"
      ],
      "metadata": {
        "id": "QTrksFwbxAbe"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Adam:\n",
        "  def __init__(self, model_weights, lr: float=0.001, beta1: float=0.4, beta2: float=0.8):\n",
        "    self.lr = lr\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.velocity = torch.zeros_like(model_weights)\n",
        "    self.accum = torch.zeros_like(model_weights)\n",
        "    self.weight = model_weights\n",
        "\n",
        "  def step(self, grad):\n",
        "    self.velocity = self.beta1 * self.velocity + (1-self.beta1) * grad\n",
        "    self.accum = self.beta2 * self.accum + (1-self.beta2) * grad ** 2\n",
        "    self.adapt_lr = self.lr / (self.accum) ** 0.5\n",
        "    self.weight = self.weight - self.adapt_lr * self.velocity"
      ],
      "metadata": {
        "id": "AtxjpXOnfCEA"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 3\n",
        "Решить задачу нахождения корней квадратного уравнения методов градиентного спуска"
      ],
      "metadata": {
        "id": "J81HIK8YoqEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Будем искать корни функции 2x**2 + 5x + 2. Для этого возведем функцию в квадрат, чтоб точки экстремума стали корнями уравнения.\n",
        "\n"
      ],
      "metadata": {
        "id": "0kaotV4WpHLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad(x):\n",
        "    # ф-ия расчета градиента\n",
        "    gradient = 2 * (2 * x ** 2 + 5 * x + 2) * (4 * x + 5)  # формула производной\n",
        "    return gradient"
      ],
      "metadata": {
        "id": "SpE5qMNKCVwf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "def grad_desc(x_start: List[int], lr: float = 0.001, stop: float = 0.00001):\n",
        "    # ф-ия градиентного спуска\n",
        "    roots = []\n",
        "    for x in x_start:\n",
        "        while True:\n",
        "            prev_x = x\n",
        "            x = x - lr * grad(x)\n",
        "            loss = grad(x) - grad(prev_x)\n",
        "            if abs(loss) <= stop:  # условие прекращения расчета при минмальном изменение градиента\n",
        "                if (2 * x ** 2 + 5 * x + 2) ** 2 <= 0.00001:  # читерское условие проверки нахождения минимума\n",
        "                    roots.append(round(x, 3))\n",
        "                    break\n",
        "                x += 0.001  # толкаем х из эксремума max к min\n",
        "    return set(roots)"
      ],
      "metadata": {
        "id": "RwPgAfh2OfoX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Корни уравнения: ', grad_desc([-5, -1.25, -1, 0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-Wf7NFSryLg",
        "outputId": "56ad9052-bd24-4999-f886-637d2155a091"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Корни уравнения:  {-0.5, -2.0}\n"
          ]
        }
      ]
    }
  ]
}